{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b617ab",
   "metadata": {},
   "source": [
    "# What is a token and why?\n",
    "\n",
    "Our goal is to model language using fundamental components and the relationships between. We want the model to be dense in terms of relationships. Having a node out by itself means it will be very unlikely to get used, but it may be very important to a topic. At the same time, we want to minimize the number of components and maximize the relationships between those components.\n",
    "\n",
    "\"Words\" will make very bad fundamental components for our model. Why? Let's just focus on English. How many words do we have?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c30e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# import for tokenizing and for the dictionary of english words\n",
    "import tiktoken\n",
    "from wordfreq import top_n_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab 100,000 most frequently used English words\n",
    "words = top_n_list('en', 100000)\n",
    "sentence = ' '.join(words)\n",
    "sys.getsizeof(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1db1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode that dictionary using GPT-2 tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokens = enc.encode(sentence)\n",
    "sys.getsizeof(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d5504",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.encode('I am writing this to demonstrate how a BPE encoder works')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6425ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in enc.encode('I am writing this to demonstrate how a BPE encoder works!'):\n",
    "    print(f\"{t:>8d} - '{enc.decode([t])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cefc302",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in enc.encode('bookkeepers keep the books for bookies who do the booking for bookers'):\n",
    "    print(f\"{t:>8d} - '{enc.decode([t])}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce0e802",
   "metadata": {},
   "source": [
    "## Building our own LLM from scratch\n",
    "...well, using PyTorch\n",
    "\n",
    "We're going to build a very small LLM using lines from country music lyrics and see how well it does writing a new line for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea2ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e2a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the rug\",\n",
    "    \"the cat chased the mouse\",\n",
    "    \"the dog chased the ball\",\n",
    "    \"the cat slept on the mat\",\n",
    "    \"the dog barked at the mailman\",\n",
    "]\n",
    "\n",
    "# corpus = [\n",
    "#     \"Picture-perfect memories scattered all around the floor\",\n",
    "#     \"Busted flat in Baton Rouge, headin' for a train. Feelin' nearly faded as my jeans\",\n",
    "#     \"I can make anybody pretty. I can make you believe any lie\",\n",
    "#     \"Stuck at a red light outside an adult bookstore. He said, 'Daddy, what are all those XXXs for?'\",\n",
    "#     \"Well, I woke up Sunday morning. With no way to hold my head that didn't hurt\",\n",
    "#     \"Becky was a beauty from South Alabama. Her daddy had a heart like a nine-pound hammer\",\n",
    "#     \"I'd gladly walk across a desert with no shoes upon my feet\",\n",
    "#     \"He had plastic bags wrapped 'round his shoes. He was covered with the evening news\",\n",
    "#     \"A candy-colored clown they call the Sandman. Tiptoes to my room every night\",\n",
    "#     \"I said, 'Grandpa, what's this picture here?  It's all black and white and ain't real clear'\",\n",
    "#     \"Maybe I didn't love you quite as often as I should have\",\n",
    "#     \"Well I'm an eight ball-shooting, double-fisted-drinking son of a gun\",\n",
    "#     \"She put him out like the burnin' end of a midnight cigarette\",\n",
    "#     \"Hello, walls. How'd things go for you today?\",\n",
    "#     \"Tumble outta bed and stumble to the kitchen. Pour myself a cup of ambition\"\n",
    "#     \"He said, 'I'll love you 'til I die'\",\n",
    "#     \"I hear the train a comin'. It's rollin' 'round the bend. And I ain't seen the sunshine since I don't know when\",\n",
    "#     \"Blame it all on my roots, I showed up in boots\",\n",
    "#     \"I wandered so aimless, life filled with sin. I wouldn't let my dear Savior in\",\n",
    "#     \"The only two things in life that make it worth livin'. Is guitars that tune good and firm-feelin' women\"\n",
    "# ]\n",
    "\n",
    "# Build word-level vocabulary\n",
    "tokens = \" \".join(corpus).split()\n",
    "vocab = sorted(set(tokens))\n",
    "stoi = {w: i for i, w in enumerate(vocab)}\n",
    "itos = {i: w for w, i in stoi.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Vocab:\", vocab)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d7b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 5  # context length (tiny on purpose)\n",
    "\n",
    "def encode(sentence):\n",
    "    return [stoi[w] for w in sentence.split()]\n",
    "\n",
    "def decode(ids):\n",
    "    return \" \".join(itos[i] for i in ids)\n",
    "\n",
    "# Build dataset of (input, target) pairs\n",
    "def build_dataset(corpus):\n",
    "    X, Y = [], []\n",
    "    for sent in corpus:\n",
    "        ids = encode(sent)\n",
    "        # slide a window over the sentence\n",
    "        for i in range(1, len(ids)):\n",
    "            # context is up to block_size tokens before position i\n",
    "            start = max(0, i - block_size)\n",
    "            x = ids[start:i]\n",
    "            y = ids[i]\n",
    "            # left-pad context with a special index (we'll use vocab_size as PAD)\n",
    "            pad_len = block_size - len(x)\n",
    "            x = [vocab_size] * pad_len + x\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "    return torch.tensor(X, dtype=torch.long), torch.tensor(Y, dtype=torch.long)\n",
    "\n",
    "X, Y = build_dataset(corpus)\n",
    "print(\"X shape:\", X.shape, \"Y shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd53a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id = vocab_size\n",
    "vocab_size_with_pad = vocab_size + 1\n",
    "print(\"Vocab size incl. PAD:\", vocab_size_with_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e426d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=32, n_heads=2, block_size=5):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(block_size, d_model)\n",
    "        \n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=n_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, block_size) of token ids\n",
    "        \"\"\"\n",
    "        B, T = x.shape\n",
    "        \n",
    "        tok_emb = self.token_embed(x)                   # (B, T, d_model)\n",
    "        pos_ids = torch.arange(T, device=x.device)      # (T,)\n",
    "        pos_emb = self.pos_embed(pos_ids)[None, :, :]   # (1, T, d_model)\n",
    "        \n",
    "        h = tok_emb + pos_emb                           # add positional info\n",
    "        \n",
    "        # Self-attention (mask not strictly needed with fixed-length windows,\n",
    "        # but we could add a causal mask for realism)\n",
    "        h_norm = self.ln1(h)\n",
    "        attn_output, attn_weights = self.attn(h_norm, h_norm, h_norm)  # (B, T, d_model)\n",
    "        h = h + attn_output                             # residual\n",
    "        \n",
    "        # Feed-forward\n",
    "        h_norm = self.ln2(h)\n",
    "        ff_output = self.ff(h_norm)                     # (B, T, d_model)\n",
    "        h = h + ff_output                               # residual\n",
    "        \n",
    "        logits = self.lm_head(h)                        # (B, T, vocab_size)\n",
    "        return logits, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c021d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model = MiniTransformer(\n",
    "    vocab_size=vocab_size_with_pad, # include PAD\n",
    "    d_model=32,\n",
    "    n_heads=2,\n",
    "    block_size=block_size,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787ff27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.to(device)\n",
    "Y_train = Y.to(device)\n",
    "\n",
    "def train(model, X, Y, epochs=300):\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        logits, _ = model(X)              # (B, T, vocab_size)\n",
    "        \n",
    "        # We only care about the **last** position in each sequence,\n",
    "        # which predicts the \"next token\" after the context window.\n",
    "        # So take logits[:, -1, :]\n",
    "        logits_last = logits[:, -1, :]    # (B, vocab_size)\n",
    "        loss = criterion(logits_last, Y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 50 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:3d} | loss = {loss.item():.4f}\")\n",
    "\n",
    "train(model, X_train, Y_train, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c73998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompt, max_new_tokens=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tokens = prompt.split()\n",
    "        for _ in range(max_new_tokens):\n",
    "            # encode last block_size tokens (or pad on the left)\n",
    "            ids = [stoi[w] for w in tokens if w in stoi]\n",
    "            ids = ids[-block_size:]\n",
    "            pad_len = block_size - len(ids)\n",
    "            x = [pad_token_id] * pad_len + ids\n",
    "            x = torch.tensor(x, dtype=torch.long, device=device)[None, :]  # (1, T)\n",
    "            \n",
    "            logits, _ = model(x)\n",
    "            logits_last = logits[:, -1, :]          # (1, vocab_size)\n",
    "            probs = F.softmax(logits_last, dim=-1)  # (1, vocab_size)\n",
    "            \n",
    "            # Sample or take argmax\n",
    "            next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "            if next_id == pad_token_id:\n",
    "                break  # avoid generating PAD token\n",
    "            \n",
    "            tokens.append(itos[next_id])\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "prompts = [\n",
    "    \"the cat\",\n",
    "    \"the dog\",\n",
    "    \"the cat chased\",\n",
    "    \"the dog sat\",\n",
    "    \"the mouse\",\n",
    "    \"the mailman\"\n",
    "]\n",
    "\n",
    "\n",
    "# prompts = [\n",
    "#     \"my heart\",\n",
    "#     \"your head\"\n",
    "# ]\n",
    "\n",
    "for p in prompts:\n",
    "    print(f\"Prompt: {p:20s} -> {generate(model, p, max_new_tokens=15)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Take one example batch row\n",
    "    sample_x = X_train[4:5]            # shape (1, block_size)\n",
    "    logits, attn_weights = model(sample_x)\n",
    "\n",
    "print(\"Context tokens:\", decode([\n",
    "    id for id in sample_x[0].tolist() if id != pad_token_id\n",
    "]))\n",
    "\n",
    "for id in sample_x[0].tolist():\n",
    "    if id != pad_token_id:\n",
    "        print(f\"{id:>5d} - {decode([id])}\")\n",
    "\n",
    "print(\"Attention weights shape:\", attn_weights.shape)  # (num_heads, B, T, T)\n",
    "attn_weights[0]  # first head's attention matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afef359",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data5740",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
