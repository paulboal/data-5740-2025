{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "tozh35alvwr7g4uzoium",
   "authorId": "1862636791025",
   "authorName": "PBOAL2",
   "authorEmail": "pboal@wustl.edu",
   "sessionId": "f0bef6aa-bc53-44c4-ab31-64b9b9159661",
   "lastEditTime": 1757653147481
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d73d884-5096-405e-88e6-f60efd369002",
   "metadata": {
    "name": "instructions",
    "collapsed": false
   },
   "source": "# Section 2: Probability and Distributions\n\nNot to be political, but I think most people would say that US Presidents Jimmy Carter and Ronald Regan were pretty different people. One was a peanut farmer. The other a movie actor. Let’s find out what their Wikipedia articles can tell us and quantify this comparison.\nFirst, we need to do some scraping of Wikipedia and parsing of text to gather the data. (Note that we’re going to oversimplify the first part a little bit for the sake of time.)\n1.\tSnowflake values security. By default you can’t make outbound internet requests from your Python code. We would have to setup and use an “external network access” integration… but Snowflake trial accounts block this feature. So, I’ve created files for you to upload into your notebook!\na.\thttps://github.com/paulboal/data-5740-2025/blob/main/Lab%2003/carter.txt\nb.\thttps://github.com/paulboal/data-5740-2025/blob/main/Lab%2003/regan.txt \n2.\tVisit each of these files. Click the “download raw” icon to download each file. Then upload each file into your notebook in Snowflake.\n3.\tTokenize the words from each article and remove common “stop words”\n4.\tCount how often each word occurs in each article, and\n5.\tCompute the probability of each word’s occurrence in each article\n6.\tFor the top five words in each article, lookup the probability of that word occurring in each article and produce a table of probabilities for those ten words.\n"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "init"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5bd6ec08-f917-4701-a4a8-1273361fb292",
   "metadata": {
    "language": "python",
    "name": "read_text"
   },
   "outputs": [],
   "source": "reagan = open('regan.txt').read()  # Forgive the misspelling\ncarter = open('carter.txt').read()\n\nlen(reagan), len(carter)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "22579335-1fe7-457b-9891-a02b2486a369",
   "metadata": {
    "language": "python",
    "name": "tokenize_text",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import re, math, collections\nstop = set(\"\"\"would but president carter reagan had carter's reagan's said a an the and or of to in for on with by from is are was were be been being as at it its this that which who whom their his her they them he she we you your our not\"\"\".split())\n\n# Our tokenize function will strip stop boards and split everything\n# that has alphabetic characters into words.\ndef tokenize(text):\n    words = re.findall(r\"[a-z']+\", text.lower())\n    return [w for w in words if w not in stop and len(w) > 2]\n\ncarter_tokens = tokenize(carter)\nreagan_tokens = tokenize(reagan)\n\n# We'll count words and compute probabilities\ndef probs(tokens):\n    c = collections.Counter(tokens) # shortcut for counting things\n    N = sum(c.values())\n    return c, {w: c[w]/N for w in c}\n\ncarter_cnt, carter_p = probs(carter_tokens)\nreagan_cnt, reagan_p = probs(reagan_tokens)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b79ea81a-29e6-4cfd-a8fb-0c2129565374",
   "metadata": {
    "language": "python",
    "name": "carter_top5",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "[x[0] for x in carter_cnt.most_common(5)]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d2ad389-6dcd-44b0-a719-62e35da3bfe2",
   "metadata": {
    "language": "python",
    "name": "reagan_top5",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "[x[0] for x in reagan_cnt.most_common(5)]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "476b0f02-baf0-42f6-9bd2-a7cc20098953",
   "metadata": {
    "language": "python",
    "name": "top5s",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "tops = pd.DataFrame(index=reagan_top5 + carter_top5, columns=[\"Carter\",\"Reagan\"])",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eafcbdfa-2472-4f69-b417-c3bf90af0f71",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "for w in tops.index:\n    tops.loc[w,\"Carter\"] = carter_p[w]\n    tops.loc[w,\"Reagan\"] = reagan_p[w]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9d07be28-ba28-49ce-8ab4-b95dec5a44b8",
   "metadata": {
    "language": "python",
    "name": "compare",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "tops",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "94d39ad5-c385-4225-9c1d-a0df0a674041",
   "metadata": {
    "language": "python",
    "name": "cosine_similarity",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Leveraging what we know about cosine_similarity...\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ncosine_similarity(\n    tops.loc[:,\"Carter\"].values.reshape(1,-1), \n    tops.loc[:,\"Reagan\"].values.reshape(1,-1)\n)",
   "execution_count": null
  }
 ]
}